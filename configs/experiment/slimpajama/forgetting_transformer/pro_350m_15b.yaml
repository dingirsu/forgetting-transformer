
# @package _global_
defaults:
  - override /model: forgetting_transformer
  - override /optimizer: adamw
  - override /schedule: warmup_cosine
  - override /datamodule: slimpajama
  - override /strategy: fsdp
  - _self_

exp: ???
tag: ???
seed: 0

output_dir: ???
data_dir: ???  # data_dir / 'longcrawl' / 'train.zarr' should exist

resume: True

log_interval: 33554432               # 32Mi. 32 times per billion
train_eval_interval: 536870912       # 512Mi
checkpoint_interval: 268435456       # 256Mi In tokens. 4 times per 1Bi. It is worth it

skip_eval: true
eval_interval: 15032385536            # 2.5Bi. Three times total
checkpoint_keep_interval: 15032385536 # 2.5Bi. Three times total

fabric:
  devices: auto
  precision: 'bf16-mixed'

train:
  max_tokens: 15032385536  # 14 Bi
  # Used for one gradient accumulation step, must be larger than batch_len
  grad_acc_tokens: 16384
  max_grad_norm: 1.0
  gradient_checkpointing: false

model:
  config:
    hidden_size: 1024
    num_hidden_layers: 24
    num_heads: 16
    use_rope: false
    rope_base: 10000
    # Pro config
    use_v_shift: true
    use_k_shift: true
    qk_norm: true
    use_output_gate: true
    use_output_norm: true
    hidden_ratio: 3.5   # output gates use extra params so we reduce it here

optimizer: 
  lr: 0.0003
  betas: [0.9, 0.95]
  weight_decay: 0.01

schedule:
  init_value: 0.00003
  peak_value: ${optimizer.lr}
  warmup_steps: 536870912  # 256Mi
  decay_steps: ${train.max_tokens}
  end_value: 0.00003

datamodule:
  train_batch_len: 2048
  train_batch_size: 256
  train_num_workers: 2

  eval_batch_len: 8192
  eval_local_batch_size: 2
  eval_tokens: 536870912  # 2Bi
  eval_num_workers: 2
