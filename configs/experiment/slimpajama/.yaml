# @package _global_
defaults:
  - override /model: transformer
  - override /optimizer: adamw
  - override /schedule: warmup_cosine
  - override /datamodule: longcrawl64
  - override /strategy: fsdp
  - _self_

exp: debug
tag: debug
seed: 0

output_dir: ???
data_dir: ???  # data_dir / 'longcrawl' / 'train.zarr' should exist

resume: True

log_interval: 2097152   # In tokens
eval_interval: 134217728  # In tokens
checkpoint_interval: 134217728  # In tokens

fabric:
  devices: auto
  precision: 'bf16-mixed'

train:
  max_tokens: 2147483648
  # Used for one gradient accumulation step, must be larger than batch_len
  grad_acc_tokens: 32768
  max_grad_norm: 1.0

model:
  config:
    hidden_size: 768
    num_hidden_layers: 12
    num_heads: 12
    initializer_range: 0.02

optimizer: 
  lr: 0.003
  betas: [0.9, 0.95]
  weight_decay: 0.1

schedule:
  init_value: 0.0
  peak_value: 0.003
  warmup_steps: 134217728  # 2 ** 27
  decay_steps: ${train.max_tokens}
  end_value: 1e-5

datamodule:
  train_batch_len: 16384
  train_seq_len: 16384
  train_doc_len: null
  train_batch_size: 16
  train_tokens_per_stage: 2147483648  # 32768 docs

  eval_batch_len: 32768
  eval_seq_len: 32768
  eval_doc_len: null
  eval_local_batch_size: 1
  eval_tokens: 134217728
